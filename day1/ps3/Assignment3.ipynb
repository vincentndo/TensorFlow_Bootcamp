{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Hyperparameters, Optimizers, and Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to be an overview of common optimizer options, regularization techniques, and a little bit on hyperparamter tuning. We show you how to make a nice api with tensorflow, use that api to make a network, and set up the network for training. \n",
    "\n",
    "From this framework, we demonstrate experiments with different optimizers and regularization techniques. We outline a few important best practices of deep learning, especially in the design phase of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow provides a nice interface for MNIST that makes it easy to load MNIST. It also provides a convenient way for us to use standard train/val/test splits.\n",
    "\n",
    "For those of you who took 189 or the decal a while ago, you might remember that these splits are delineated as so:\n",
    "* train: the data we train on \n",
    "* val:   the data we use to adjust our hyperparameters\n",
    "* test:  the data we use at the very end of our hyperparameter search to report the quality of our model. Shouldn't be looked at until you are done tuninig hyperparameters and never should be a metric to adjust a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate\n",
    "Here I've prepared a few boilerplate functions that I like to use to make construction of nets easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relu = tf.nn.relu\n",
    "def weight_variable(shape,name=None):\n",
    "    initial = # ___ YOUR CODE HERE ____\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape,name=None):\n",
    "    initial = # ___ YOUR CODE HERE ____\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def fc_layer(x, W, b, act_fn=None):\n",
    "    if act_fn is not None:\n",
    "        # return the layer function (affine operation) - make sure you apply the act_fn\n",
    "        return # ___ YOUR CODE HERE ____\n",
    "    else:\n",
    "        #return the affine function by itself\n",
    "        return # ___ YOUR CODE HERE ____\n",
    "def accuracy(preds, true):\n",
    "    #compute accuracy using predicted and true\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(preds, 1), tf.argmax(true, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've defined everything correctly, then this test should pass and you won't receive an assertion error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_layers():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    x = tf.random_normal([1,10])\n",
    "    w = weight_variable([10,10])\n",
    "    b = bias_variable([10])\n",
    "    layer1 = fc_layer(x, w, b, act_fn=relu)\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        output = sess.run(layer1)\n",
    "        print(output)\n",
    "        assert np.allclose(output,np.array([[ 0.17964056,  0.1712192 ,  0.0615531 ,  0.04394994,  0.,0.06671001,  0.36768118,  0.11596319,  0.,  0.03804523]], dtype=np.float32))\n",
    "test_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the input\n",
    "We define the input before anything else because all of this will stay constant throughout all of our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the placeholders for the dataset. X is the features and y are the labels\n",
    "x =  # ___ YOUR CODE HERE ____\n",
    "y_ =  # ___ YOUR CODE HERE ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a simple Fully Connected Net\n",
    "Here you should build a fully-connected net. Read the doc string for more details.\n",
    "\n",
    "Use the `layers` dictionary to return all of the layers you defined. \n",
    "\n",
    "Additionally, make sure that you set `logits` the output of the last layer BEFORE applying an `act_fn`. This means you'll have to set `act_fn` to `None`.\n",
    "\n",
    "\n",
    "Make sure that you return `logits` as your first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_net(x):\n",
    "    '''\n",
    "    Simple Fully-connected network for MNIST\n",
    "    Input: 784\n",
    "    Hidden Layers\n",
    "        (1): 800\n",
    "        (2): 300\n",
    "    Output: 10, softmax output\n",
    "    \n",
    "    Returns:\n",
    "    tuple with the contents `(logits, layers)`\n",
    "    \n",
    "    logits: the op corresponding to the last fully-connect layer BEFORE applying softmax\n",
    "    layers: dictionary containing all of the layers\n",
    "    '''\n",
    "    layers = {\n",
    "        'input': x,\n",
    "    }# (Ignore if you don't understand this yet) COPY Below this line, including return\n",
    "    \n",
    "    # define layer 1 parameters here \n",
    "    layers['fc1'] = # ___ YOUR CODE HERE ____\n",
    "    \n",
    "    # define layer 2 parameters here \n",
    "    layers['fc2'] = # ___ YOUR CODE HERE ____\n",
    "    \n",
    "    # define layer 3 parameters here \n",
    "    layers['fc3'] = # ___ YOUR CODE HERE ____\n",
    "    \n",
    "    layers['logits'] = logits = layers['fc3']\n",
    "    layers['pred'] = tf.nn.softmax(layers['logits'])\n",
    "    return logits, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the rest of the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits, net = fc_net(x)\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "acc = accuracy(net['pred'], y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring different optimizers\n",
    "\n",
    "In this section we look at different optimizers. They all stem from [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) originally.\n",
    "\n",
    "but there are many modified optimizers that use statistics about the preceding learning rates that happen to help inform the optimizer's function.\n",
    "\n",
    "We look in particular at RMSProp and Adam- two typically high performoing, and thus desired optimization techniques. There are many others that have performed very well in the past and may hold promise in the future. A good summary is compiled in this [video](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "## Standard SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start with the standard SGD Optimizer. Try with a learnsing rate of 1e-2\n",
    "optimizer = # ___ YOUR CODE HERE ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        n_batches = (int) (mnist.train.num_examples/batch_size)\n",
    "        for e in range(n_batches):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            optimizer.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "        # calculate validation accuracy every 5th epoch\n",
    "        if i % 1 == 0:\n",
    "            val_batch = mnist.validation.next_batch(mnist.validation.num_examples)\n",
    "            train_batch = mnist.train.next_batch(1000)\n",
    "            train_accuracy = acc.eval(feed_dict={\n",
    "                x: train_batch[0], y_: train_batch[1]\n",
    "            })\n",
    "\n",
    "            val_accuracy = acc.eval(feed_dict={\n",
    "                x: val_batch[0], y_: val_batch[1]\n",
    "            })\n",
    "            train_acc_list.append(train_accuracy)\n",
    "            val_acc_list.append(val_accuracy)\n",
    "            print('epoch %d \\ttraining accuracy %g \\tvalidation accuracy %g' % (i, train_accuracy, val_accuracy))\n",
    "       \n",
    "\n",
    "    print('test accuracy %g' % acc.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xaxis = np.arange(num_epochs)\n",
    "plt.plot(Xaxis, train_acc_list, color='r', label='train acc')\n",
    "plt.plot(Xaxis, val_acc_list,color='g', label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "learning_rate=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with RMSProp Optimizer. You should search the tensorflow documentation to find it.\n",
    "# use the learning_rate parameter from the previous cell\n",
    "optimizer = # ___ YOUR CODE HERE ____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        n_batches = (int) (mnist.train.num_examples/batch_size)\n",
    "        for e in range(n_batches):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            optimizer.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "        # calculate validation accuracy every 5th epoch\n",
    "        if i % 1 == 0:\n",
    "            val_batch = mnist.validation.next_batch(mnist.validation.num_examples)\n",
    "            train_batch = mnist.train.next_batch(1000)\n",
    "            train_accuracy = acc.eval(feed_dict={\n",
    "                x: train_batch[0], y_: train_batch[1]\n",
    "            })\n",
    "            val_accuracy = acc.eval(feed_dict={\n",
    "                x: val_batch[0], y_: val_batch[1]\n",
    "            })\n",
    "            print('epoch %d \\ttraining accuracy %g \\tvalidation accuracy %g' % (i, train_accuracy, val_accuracy))\n",
    "       \n",
    "\n",
    "    print('test accuracy %g' % acc.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate the [ADAM regularizor](https://arxiv.org/pdf/1412.6980.pdf). This regularizer uses the mean and the variance of the gradients, updated iteratively using a bias constant term and the gradient of the network. This produces a supposedly much more smooth update to movement in the hyperspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now setup an ADAM optimizer. Look in the TensorFlow documentation for more details.\n",
    "# use the learning_rate parameter from the previous cell\n",
    "optimizer = # ___ YOUR CODE HERE ____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        n_batches = (int) (mnist.train.num_examples/batch_size)\n",
    "        for e in range(n_batches):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            optimizer.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "        # calculate validation accuracy every 5th epoch\n",
    "        if i % 1 == 0:\n",
    "            val_batch = mnist.validation.next_batch(mnist.validation.num_examples)\n",
    "            train_batch = mnist.train.next_batch(1000)\n",
    "            train_accuracy = acc.eval(feed_dict={\n",
    "                x: train_batch[0], y_: train_batch[1]\n",
    "            })\n",
    "\n",
    "            val_accuracy = acc.eval(feed_dict={\n",
    "                x: val_batch[0], y_: val_batch[1]\n",
    "            })\n",
    "            train_acc_list.append(train_accuracy)\n",
    "            val_acc_list.append(val_accuracy)\n",
    "            print('epoch %d \\ttraining accuracy %g \\tvalidation accuracy %g' % (i, train_accuracy, val_accuracy))\n",
    "       \n",
    "\n",
    "    print('test accuracy %g' % acc.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "Here we demonstrate the effects of several different regularization techniques. The idea behind these techniques is that they are all intended to reduce the separation between training accuracy and validation accuracy. They help to prevent the training steps from overfitting to the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 /L2 regularization\n",
    "This is probably the simplest form of regularization. In this case, we add a simple l2-norm constant to the cost function we try to learn. We borrow this regularization from the Ridge Classiciation [L2-Norm] and Lasso [L1-Norm], which were the same idea, but with a logistic regression function.\n",
    "\n",
    "In this we have to add a term to the loss function, which is relatively straight forward to understand and use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_l_norm(x, reg=tf.nn.l2_loss):\n",
    "    layers = {\n",
    "        'input': x,\n",
    "    }\n",
    "\n",
    "    # COPY from fc_net here \n",
    "    # Change weight1, weight2 ... to whatever your weight variables are called.\n",
    "    # TODO move this line to before the RETURN statement\n",
    "    # layers['regularizer'] = reg(weight1) + reg(weight2) + reg(weight3) \n",
    "\n",
    "logits, net = fc_l_norm(x, tf.nn.l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "\n",
    "# the constant coefficient of the regularization part of the loss\n",
    "reg_alpha = 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits)\n",
    "regularizer = net['regularizer']\n",
    "# make sure that you include the regularizer to the loss function. Make sure that it's scaled by `reg_alpha`\n",
    "loss = tf.reduce_mean(entropy)# ___ YOUR CODE HERE ____\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "net_output = tf.nn.softmax(logits)\n",
    "acc = accuracy(net_output, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "dropout_prob = 1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        n_batches = (int) (mnist.train.num_examples/batch_size)\n",
    "        for e in range(n_batches):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            optimizer.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "        # calculate validation accuracy every 5th epoch\n",
    "        if i % 1 == 0:\n",
    "            val_batch = mnist.validation.next_batch(mnist.validation.num_examples)\n",
    "            train_batch = mnist.train.next_batch(1000)\n",
    "            train_accuracy = acc.eval(feed_dict={\n",
    "                x: train_batch[0], y_: train_batch[1]\n",
    "            })\n",
    " \n",
    "            val_accuracy = acc.eval(feed_dict={\n",
    "                x: val_batch[0], y_: val_batch[1]\n",
    "            })\n",
    "            print('epoch %d \\ttraining accuracy %g \\tvalidation accuracy %g' % (i, train_accuracy, val_accuracy))\n",
    "       \n",
    "\n",
    "    print('test accuracy %g' % acc.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "[Srivistava et. al.](http://jmlr.org/papers/v15/srivastava14a.html)\n",
    "Dropout provides an extremely interesting case of regularization. The basis for this technique relies \n",
    "on this inspection of a neural network. When we observe the information that a \n",
    "neuron in the $k$th layer of a $n$ layer neural may begin relying heavily on a few neurons in \n",
    "the $k-1$th layer. In turn those $k-1$ neurons rely on $k-2$ neurons and so on. This reliance on certain weights over others causes the model to be much more fragile. Certain misleading signals that come from the input may be multiplied through the series of chains - a result of overfitting on the dataset. \n",
    "\n",
    "Dropout attempts to prevent this issue by randomly preventing weights from contributing to a feed-forward and updated by the subsequent backprop. A practitioner sets the dropout parameter as a probability that a neuron will not be included in the feedforward. This helps the model to learn a representation that is not very reliant on any single neuron, but rather upon many, increasing the chances that more general information will be learned. \n",
    "\n",
    "\n",
    "![dropout.jpeg](dropout.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copy your code from fc_net and add it here. then move the line that starts with layer['dropout1'] in between 'fc1' and 'fc2'\n",
    "# so that dropout is in between fc1 and fc2\n",
    "def fc_net_dropout(x, keep_prob_op):\n",
    "    layers = {\n",
    "        'input': x,\n",
    "        'keep_prob': keep_prob_op\n",
    "    }\n",
    "\n",
    "    # TODO put between two layers\n",
    "    # layers['dropout1'] = tf.nn.dropout(\"TODO CHANGE WITH INPUT LAYER\", layers['keep_prob'])\n",
    "    # COPY fc_net() below the layer dict initialization\n",
    "# only useful for dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "net, net_dict = fc_net_dropout(x, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=net)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "net_output = tf.nn.softmax(net)\n",
    "acc = accuracy(net_output, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "dropout_prob = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_acc_dropout_list = []\n",
    "val_acc_dropout_list = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        n_batches = (int) (mnist.train.num_examples/batch_size)\n",
    "        for e in range(n_batches):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            optimizer.run(feed_dict={x: batch[0], y_: batch[1],  net_dict['keep_prob'] : dropout_prob})\n",
    "        # calculate validation acc every 5th epoch\n",
    "        if i % 1 == 0:\n",
    "            val_batch = mnist.validation.next_batch(mnist.validation.num_examples)\n",
    "            train_batch = mnist.train.next_batch(1000)\n",
    "            train_acc = acc.eval(feed_dict={\n",
    "                x: train_batch[0], y_: train_batch[1], net_dict['keep_prob'] : 1\n",
    "            })\n",
    "            #val_acc = acc.eval(feed_dict={\n",
    "           #     x: mnist.validation.images, y_: mnist.validation.labels\n",
    "           # })\n",
    "            val_acc = acc.eval(feed_dict={\n",
    "                x: val_batch[0], y_: val_batch[1], net_dict['keep_prob'] : 1\n",
    "            })\n",
    "            train_acc_dropout_list.append(train_acc)\n",
    "            val_acc_dropout_list.append(val_acc)\n",
    "            print('epoch %d \\ttraining accuracy %g \\tvalidation accuracy %g' % (i, train_acc, val_acc))\n",
    "       \n",
    "\n",
    "    print('test accuracy %g' % acc.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels, net_dict['keep_prob']: 1}))#, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's important that we compare the results of training using droput with regular training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xaxis = np.arange(num_epochs)\n",
    "plt.plot(Xaxis, train_acc_list, color='r', label='train acc')\n",
    "plt.plot(Xaxis, train_acc_dropout_list, color='b', label='train acc dropout')\n",
    "plt.plot(Xaxis, val_acc_list,color='g', label='val acc')\n",
    "plt.plot(Xaxis, val_acc_dropout_list,color='y', label='val acc dropout')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we procede with Adam for optimization for it's simplicity and general first choice. We recommend as an exercise to try RMSProp and plot the curve as well in comparison to Adam. That way, you'll see how the learning rate evolves over time.\n",
    "## Batch Normalization\n",
    "[Batch Normaliazation ](https://arxiv.org/abs/1502.03167)is a regularization technique that is meant to preserve the statistics inside of a network. The batch normalization technique basically ensures that a layers output would match a certain mean and standard deviation. This technique stabilizes parameter growth, enabling higher learning rates. This also happens to add normalization to the model as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one is a little bit harder. The syntax for batch normalization is similar to dropout.\n",
    "# we just add it as a part of our graph. Try adding it to the input of fc2. You can also try adding it \n",
    "# to different parts of the net as well.\n",
    "def fc_batch_norm(x):\n",
    "    layers = {\n",
    "        'input': x,\n",
    "    }\n",
    "    # COPY fc_net() below the layer dict initialization\n",
    "net, net_dict = fc_batch_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the gradients are smaller, you should try a few different learning rates to see how that effects the training of the model. It's a lot easier to compare if you write the information of each run inside of a variable or a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "net_output = tf.nn.softmax(logits)\n",
    "acc = accuracy(net_output, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_acc_bn_list = []\n",
    "val_acc_bn_list = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        n_batches = (int) (mnist.train.num_examples/batch_size)\n",
    "        for e in range(n_batches):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            optimizer.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            \n",
    "        # calculate validation accuracy every 5th epoch\n",
    "        if i % 1 == 0:\n",
    "            val_batch = mnist.validation.next_batch(mnist.validation.num_examples)\n",
    "            train_batch = mnist.train.next_batch(1000)\n",
    "            \n",
    "            train_accuracy = acc.eval(feed_dict={\n",
    "                x: train_batch[0], y_: train_batch[1]\n",
    "            })\n",
    "\n",
    "            val_accuracy = acc.eval(feed_dict={\n",
    "                x: val_batch[0], y_: val_batch[1]\n",
    "            })\n",
    "            train_acc_bn_list.append(train_accuracy)\n",
    "            val_acc_bn_list.append(val_accuracy)\n",
    "            \n",
    "            print('epoch %d \\ttraining accuracy %g \\tvalidation accuracy %g' % (i, train_accuracy, val_accuracy))\n",
    "       \n",
    "\n",
    "    print('test accuracy %g' % acc.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels}))#, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_acc_bn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xaxis = np.arange(num_epochs)\n",
    "plt.plot(Xaxis, train_acc_list, color='r', label='train acc')\n",
    "plt.plot(Xaxis, train_acc_dropout_list, color='b', label='train acc dropout')\n",
    "plt.plot(Xaxis, train_acc_bn_list, color='k', label='train acc bn')\n",
    "plt.plot(Xaxis, val_acc_list,color='g', label='val acc')\n",
    "plt.plot(Xaxis, val_acc_dropout_list,color='y', label='val acc dropout')\n",
    "plt.plot(Xaxis, val_acc_bn_list,color='#e12f12', label='val acc bn')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End Note\n",
    "Now that you've seen a few of these techniques in action, you might be wondering how you might have noticed that there was a lot of redundancy in our code. Additionally, you might've found it kind of annoying to compare the results for something trained using batch norm with another net trained with dropout instead. You also may have noticed that we separated regularization techniques from optimization techniques like Adam. \n",
    "\n",
    "It turns out that Adam actually has some regularization properties - the variance and mean of the network parameters regularize the gradients of the model. It's important to think of all the different ways you are thinking about your model when you're experimenting with a new technique. If you know all of the knobs and how they work you'll be able to find new patterns that work very well. This is a tedious process, but we want to show you how to manage this large scale of a task with more convenient methods than those that are demonstrated here.\n",
    "\n",
    "In the next assignment, we look at a different way to display hyperparmeter tuning results using TensorBoard. This allows you to see the results of each experiment on the exact same plot. The tool is extremely useful for all of your deep learning needs. It will be the resource you use to debug your nets.\n",
    "\n",
    "Additionally, we've included a copy of this assignment, but using CNN models instead. We've taken the liberty and condensed the CNN variations into a single model that's dictated by the parameters. We invite you to try to play around wiht different configurations of the CNN. You could add more fc layers, expand the size of some layers, or play around with different convolutional filters (I'd recommend that order in fact). \n",
    "\n",
    "If you have an idea that you think is cool with this stuff, hit me up at philkuz@ml.berkeley.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
