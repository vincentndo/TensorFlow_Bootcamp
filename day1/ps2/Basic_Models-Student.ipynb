{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Models in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Dummy Data\n",
    "We're going to build a simple linear regression model that learns a function between 1d input and 1d output. \n",
    "\n",
    "This means for scalar input $X$ and scalar output $Y$, we learn a function the weights of the function $f(x)$\n",
    "\n",
    "$$f(X) = w * X + b$$ where we learn $w, b$\n",
    "\n",
    "We're going to use a dummy dataset for ease. We load the csv in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"linreg.csv\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a placeholder value for the feature X and the output y. Both X and y should be float scalars represented by tf.float32. The placeholders set the type and shape of input. By default, placeholders are scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "X = tf.placeholder(...)\n",
    "y = tf.placeholder(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable for the weight, $w$, and another for the bias, $b$. Both can initially be set to a small number such as 0.1. These will be the values we update in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "w = tf.Variable(...)\n",
    "b = tf.Variable(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply compute the predicted value of y using $\\hat{y}  = w * X + b$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.add(tf.multiply(w, X), b) \n",
    "# We could have also written w* X + b but using tf methods can make the operation more clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the loss. We're using Mean Squared Error for our  loss function, $$\\mathcal{L}_{MSE} = (y - \\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "loss = tf.square(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "The optimizer powers our algorithm's learning abilities. The optimizer performs gradient descent on the loss function, and adjusts all of the variables in order to decrease the loss.\n",
    "\n",
    "We will use the simplest optimizer, GradientDescentOptimizer, with an appropriate learning rate. Set it to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate= ... ).minimize(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "* For each epoch, run the optimizer on each X,y pair (each batch) from the dataset and sum up the loss over all data points\n",
    "* Print the loss after each epoch (full iteration through dataset)\n",
    "* Get the final values of the weights and the bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(5):\n",
    "    total_loss = 0\n",
    "    for row in df.values:\n",
    "        x_batch = row[0]\n",
    "        y_batch = row[1]\n",
    "        #TODO: Fill in the dots\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={X: ..., y: ...})\n",
    "        total_loss += l\n",
    "    # Print the loss after each epoch ()\n",
    "    print(\"Epoch {0}: {1}\".format(i, total_loss))\n",
    "# Get the final values of the weights and the bias \n",
    "weight, bias = sess.run([w, b]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** In the last code block, if your loss gradually increased, stayed the same, or return NaN, then go back to the optimizer and lower the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to plot the data and the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df.values[:,0], df.values[:,1])\n",
    "plt.plot(df.values[:,0], weight*df.values[:,0]+bias, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression Using MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow provides a convenient interface for MNIST data. This makes it really easy to test your code on a dataset that is commonly used. The code below shows you how to read MNIST images and store the labels as one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST = input_data.read_data_sets(\"../data/mnist\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create placeholders for X and Y. \n",
    "* Note that each MNIST image is 28x28. Additionally, the data will already be flattened into a 784 dimensional vector when we input it into the model\n",
    "* Each label is 10d - a vector element for every possible digit.\n",
    "* Make sure the shapes of the placeholders are defined so a variable number of images and labels can be fed in each batch. *This is what index 0 manages. Just put None instead of a dimension in this piece of the net*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "X = tf.placeholder (...)\n",
    "Y = tf.placeholder (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a weights variable and a biases variable of the appropriate shapes.\n",
    "* Initialize the weights variable from a truncated normal distribution using tf.truncated_normal(...) - this is better than setting weights to zero because it removes symmetry from backpropagation. [Here's a more in depth discussion](https://datascience.stackexchange.com/a/10930)\n",
    "* The bias variable should also be set to a small value, such as 0.1. Do this by using tf.constant(...) and inputting the value and the appropriate shape\n",
    "* When you multiply the feature vector X and the weights variable, the result should be the same shape as the bias tensor so they can be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "W = tf.Variable (tf.truncated_normal(...))\n",
    "b = tf.Variable(tf.constant(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the logits as $X \\cdot W + b$. Logits are a quantity that will be mapped to a probability with softmax. This is the exact same expression as multi-dimensional linear regression.\n",
    "* Make sure to use tf.matmul() when multiplying matrices. Using \\* will multiply element wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "logits = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the entropy using tf.nn.softmax_cross_entropy_with_logits(...). This will apply the softmax function to the logits before calculating the entropy. Compute the loss as the mean over the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels = ...)\n",
    "loss = tf.reduce_mean(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the optimizer as the GradientDescentOptimizer with an appropriate learning rate. *(Hint: Try 0.01).* Set it to minimize the loss.\n",
    "* Note: When running the optimizer, if the loss is nan or increasing with each epoch, try decreasing the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy by:\n",
    "* using tf.equal on the predicted label and the true label\n",
    "* casting that to a float and computing the mean over all examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "Y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = tf.argmax(Y_pred, 1)\n",
    "y_cls = ...\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(..., ...), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start an Interactive Session and initialize all the global variables.\n",
    "* For each epoch, run the optimizer on each X,y pair and sum up the loss over all data points\n",
    "* Print the loss after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "batch_size = 128\n",
    "# initialize all the global variables\n",
    "sess.run(...)\n",
    "n_batches = (int) (MNIST.train.num_examples/batch_size)\n",
    "for i in range(25):\n",
    "    total_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # iterate through the batches \n",
    "        X_batch, y_batch = MNIST.train.next_batch(batch_size)\n",
    "        o, l = sess.run(...)\n",
    "        total_loss += l\n",
    "    print(\"Epoch {0}; Loss: {1}\".format(i, total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the accuracy by feeding in all the test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "\n",
    "print(\"Computing accuracy ...\")\n",
    "X_batch, y_batch = MNIST.test.next_batch(MNIST.test.num_examples)\n",
    "final_accuracy = sess.run(...)\n",
    "\n",
    "print (\"Accuracy {0}\".format(final_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Feed Forward Network on MNIST\n",
    "Here we build a simple fully-connected network for MNIST. The network will have 2 hidden layers:  784 input neurons (28x28 shaped mnist), 2x layers with 256 hidden neurons , and 10 output neurons ( 1 for each digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create placeholders for X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "X = tf.placeholder(...)\n",
    "Y = tf.placeholder(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare each layer in the network and the final logits by:\n",
    "* Creating variables for weights and biases of the appropriate sizes\n",
    "* Applying ReLu on $X \\cdot W + b$\n",
    "\n",
    "This formulation is nearly the same as the logistic regression setup - except instead of applying softmax on the output of the hidden layers, we apply [relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "\n",
    "Network Configurations:\n",
    "* First layer has 784 input features and 256 output features\n",
    "* Second layer has 256 input features and 256 output features\n",
    "* Third layer has 256 input features and 10 output features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal(...))\n",
    "b1 = tf.Variable(tf.constant(...))\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(...)\n",
    "b2 = tf.Variable(tf.constant(...))\n",
    "layer2 = tf.nn.relu(...)\n",
    "\n",
    "W_out = tf.Variable(...)\n",
    "b_out = tf.Variable(tf.constant(...))\n",
    "logits = tf.matmul(...) + ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the entropy using tf.nn.softmax_cross_entropy_with_logits. This will apply the softmax function to the logits before calculating the entropy. The loss as the mean over the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels=...)\n",
    "loss = tf.reduce_mean(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the optimizer as the GradientDescentOptimizer with an appropriate learning rate. Set it to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy by:\n",
    "* using tf.equal on the predicted label and the true label\n",
    "* casting that to a float and computing the mean over all examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "\n",
    "Y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = ...\n",
    "y_cls = ...\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(...), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start an Interactive Session and initialize all the global variables.\n",
    "* For each epoch, run the optimizer on each X,y batch and sum up the loss over all data points\n",
    "* Print the loss after each epoch\n",
    "\n",
    "We set the batch size to 128 and epochs to 25. Feel free to play around with these variables. Additionally, every 5 epochs we calculate validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(...)\n",
    "\n",
    "n_batches = (int) ...\n",
    "for i in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        X_batch, y_batch = ...\n",
    "        o, l = sess.run(...)\n",
    "        total_loss += l\n",
    "    print(\"Epoch {0}: {1}\".format(i, total_loss))\n",
    "    if i % 5 == 0 and i!= 0:\n",
    "        X_val, y_val = MNIST.validation.next_batch(MNIST.validation.num_examples)\n",
    "        val_accuracy = sess.run(...)\n",
    "        print(\"\\tVal Accuracy {0}\".format(val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training and all validation, you'll want to return your test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Fill in the dots\n",
    "\n",
    "print(\"Computing accuracy ...\")\n",
    "X_batch, y_batch = MNIST.test.next_batch(MNIST.test.num_examples)\n",
    "final_accuracy = sess.run(...)\n",
    "\n",
    "print (\"Test Accuracy {0}\".format(final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
