{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from data_utils import minibatches, pad_sequences, get_chunks\n",
    "from general_utils import Progbar, print_sentence\n",
    "\n",
    "from data_utils import get_trimmed_glove_vectors, load_vocab, \\\n",
    "    get_processing_word, CoNLLDataset\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Tagging (Named Entity Recognition) with Tensorflow:\n",
    "Implement a sequence tagging model using tensorflow. Apparently this demo has state-of-the-art performance (F1 score close to 91) given a large enough corpus.\n",
    "\n",
    "## Dependencies\n",
    "https://github.com/stanfordnlp/GloVe\n",
    "## Task\n",
    "\n",
    "Given a sentence, give a tag to each word. A classical application is Named Entity Recognition (NER). \n",
    "\n",
    "According to Wikipedia:\n",
    "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "```\n",
    "John  lives in New   York  and works for the European Union\n",
    "B-PER O     O  B-LOC I-LOC O   O     O   O   B-ORG    I-ORG\n",
    "```\n",
    "\n",
    "## Entity Notation\n",
    "\n",
    "In the CoNLL2003 task, the entities are LOC, PER, ORG and MISC for locations, persons, orgnizations and miscellaneous. The no-entity tag is O. Because some entities (like New York) have multiple words, we use a tagging scheme to distinguish between the beginning (tag B-...), or the inside of an entity (tag I-...). Other tagging schemes exist (IOBES, etc). Tagging schemes are arbitrarily determined.\n",
    "\n",
    "\n",
    "## Very High Level Overview of Model\n",
    "\n",
    "Word Representation: Word vector w \\in R^{n}. In this case, we're using GloVe, but other word embeddings include Word2Vec, Senna, etc.\n",
    "\n",
    "Contextual Word Representation: Use an LSTM to represent for each word in its context (via hidden layers).\n",
    "\n",
    "Decoding: Once we have a vector representing each word, we can use it to make a prediction.\n",
    "\n",
    "## Lower Level Overview of Model\n",
    "Similar to [Lample et al.](https://arxiv.org/abs/1603.01360) and [Ma and Hovy](https://arxiv.org/pdf/1603.01354.pdf).\n",
    "\n",
    "- concatenate final states of a bi-lstm on character embeddings to get a character-based representation of each word\n",
    "- concatenate this representation to a standard word vector representation (GloVe here)\n",
    "- run a bi-lstm on each sentence to extract contextual representation of each word\n",
    "- decode with a linear chain CRF\n",
    "\n",
    "## Steps\n",
    "Follow Step 1., ..., Step n. to complete the model.\n",
    "\n",
    "blog post as reference: https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NERModel(object):\n",
    "    def __init__(self, config, embeddings, ntags, nchars=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: class with hyper parameters\n",
    "            embeddings: np array with embeddings\n",
    "            nchars: (int) size of chars vocabulary\n",
    "        \"\"\"\n",
    "        self.config     = config\n",
    "        self.embeddings = embeddings\n",
    "        self.nchars     = nchars\n",
    "        self.ntags      = ntags\n",
    "        self.logger     = config.logger # now instantiated in config\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"\n",
    "        Adds placeholders to self\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Step 1.a. Since Tensorflow recieves batches of words and data, pad sentences to\n",
    "        make them the same length. define two placeholders (self.word_ids, self.sequence_lengths).\n",
    "        We pad according to the max length of a word in data/ and max length of a sentence in data/.\n",
    "        This is already been done for the sake of syntax.\n",
    "        \"\"\"\n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_ids\")\n",
    "\n",
    "        # shape = (batch size)\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                        name=\"sequence_lengths\")\n",
    "\n",
    "        # shape = (batch size, max length of sentence, max length of word)\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None],\n",
    "                        name=\"char_ids\")\n",
    "\n",
    "        # shape = (batch_size, max_length of sentence)\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_lengths\")\n",
    "\n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"labels\")\n",
    "\n",
    "        # hyper parameters\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"dropout\")\n",
    "        self.lr = tf.placeholder(dtype=tf.float32, shape=[], \n",
    "                        name=\"lr\")\n",
    "\n",
    "\n",
    "    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n",
    "        \"\"\"\n",
    "        Given some data, pad it and build a feed dictionary\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of words. \n",
    "                A word is a list of ids\n",
    "            labels: list of ids\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "        \"\"\"\n",
    "        # perform padding of the given data\n",
    "        if self.config.chars:\n",
    "            char_ids, word_ids = zip(*words)\n",
    "            word_ids, sequence_lengths = pad_sequences(word_ids, 0)\n",
    "            char_ids, word_lengths = pad_sequences(char_ids, pad_tok=0, nlevels=2)\n",
    "        else:\n",
    "            word_ids, sequence_lengths = pad_sequences(words, 0)\n",
    "\n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            self.word_ids: word_ids,\n",
    "            self.sequence_lengths: sequence_lengths\n",
    "        }\n",
    "\n",
    "        if self.config.chars:\n",
    "            feed[self.char_ids] = char_ids\n",
    "            feed[self.word_lengths] = word_lengths\n",
    "\n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "\n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "\n",
    "        return feed, sequence_lengths\n",
    "\n",
    "    \"\"\"\n",
    "    Step 1.b. Use a bi-LSTM for feature extraction of words at the n-gram level\n",
    "    (so output a word embedding for each word).\n",
    "    \"\"\"\n",
    "    def add_word_embeddings_op(self):\n",
    "        \"\"\"\n",
    "        Adds word embeddings to self\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            _word_embeddings = tf.Variable(self.embeddings, name=\"_word_embeddings\", dtype=tf.float32, \n",
    "                                trainable=self.config.train_embeddings)\n",
    "            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, \n",
    "                name=\"word_embeddings\")\n",
    "\n",
    "        with tf.variable_scope(\"chars\"):\n",
    "            if self.config.chars:\n",
    "                \"\"\"\n",
    "                Initialize an embedding matrix that is like a dictionary of\n",
    "                character: letter vector. \n",
    "                Set the shape of the output _char_embeddings accordingly to:\n",
    "                self.nchars- The number of characters available in vocab.\n",
    "                    This is a predefined instance variable.\n",
    "                self.config.dim_char- The dimension of a character vector.\n",
    "                    This is a predefined instance variable.\n",
    "                    \n",
    "                char_embeddings is the actual matrix to look up the letter vectors.\n",
    "                \"\"\"\n",
    "                _char_embeddings = tf.get_variable(name=\"_char_embeddings\", dtype=tf.float32, \n",
    "                    shape=['-----', '-----'])\n",
    "                # shape = (batch, sentence, word, dim of char embeddings)\n",
    "                char_embeddings = tf.nn.embedding_lookup(_char_embeddings, self.char_ids, \n",
    "                    name=\"char_embeddings\")\n",
    "                \n",
    "                \"\"\"\n",
    "                Reshape our 4-dimensional tensor to match the requirement of bidirectional_dynamic_rnn.\n",
    "                This has been done for you because messing with shapes is sad.\n",
    "                \"\"\"\n",
    "                # put the time dimension on axis=1 for dynamic_rnn\n",
    "                s = tf.shape(char_embeddings) # store old shape\n",
    "                # shape = (batch x sentence, word, dim of char embeddings)\n",
    "                char_embeddings = tf.reshape(char_embeddings, shape=[-1, s[-2], self.config.dim_char])\n",
    "                word_lengths = tf.reshape(self.word_lengths, shape=[-1])\n",
    "                \"\"\"\n",
    "                Initialize a bi lstm on characters. Make\n",
    "                cell_fw: An instance of tf.contrib.rnn.LSTMCell, to be used for forward direction.\n",
    "                cell_bw: An instance of tf.contrib.rnn.LSTMCell, to be used for backward direction.\n",
    "                \n",
    "                where hidden state size is self.config.char_hidden_size.\n",
    "                The state of the lstm should a tuple of memory and hidden state.\n",
    "                \"\"\"\n",
    "                # need 2 instances of cells since tf 1.1\n",
    "                cell_fw = '-----'\n",
    "                cell_bw = '-----'\n",
    "\n",
    "                _, ((_, output_fw), (_, output_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                    cell_bw, char_embeddings, sequence_length=word_lengths, \n",
    "                    dtype=tf.float32)\n",
    "\n",
    "                output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "                # shape = (batch size, max sentence length, char hidden size)\n",
    "                output = tf.reshape(output, shape=[-1, s[1], 2*self.config.char_hidden_size])\n",
    "\n",
    "                word_embeddings = tf.concat([word_embeddings, output], axis=-1)\n",
    "\n",
    "        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout)\n",
    "\n",
    "    \"\"\"\n",
    "    Step 2.a. Decoding\n",
    "    Computing Tags Scores: At this stage, each word w is associated with a vector h that \n",
    "    captures information from the meaning of the word, its characters and its context.\n",
    "    \"\"\"\n",
    "    def add_logits_op(self):\n",
    "        \"\"\"\n",
    "        Adds logits to self\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Here is a bi-lstm for your bi-lstm just in case you didn't have enough.\n",
    "        Predicts the output vector for each word embedding. Like characters,\n",
    "        words are read sequentially so it'd also be a good idea to \n",
    "        analyze them with a bi-lstm. \n",
    "        \n",
    "        Create\n",
    "        cell_fw, cell_bw: Instances of tf.contrib.rnn.LSTMCell.\n",
    "            To be used for forward direction.\n",
    "            Hidden state size = self.config.hidden_size\n",
    "        (output_fw, output_bw), _: outputs of tf.nn.bidirectional_dynamic_rnn.\n",
    "            Like the bi-lstm for char embeddings, the bi-lstm of word embeddings\n",
    "            should include cell_fw, cell_bw, input, sequence length (sentence length).\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = '-----'\n",
    "            cell_bw = '-----'\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn('-----', \n",
    "                '-----', '-----', sequence_length='-----', \n",
    "                dtype=tf.float32)\n",
    "            # Stores the output as a tuple of the forward and backward direction\n",
    "            # output states\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            \"\"\"\n",
    "            Implement dropout with tf.nn.dropout with hyperparameter, self.dropout.\n",
    "            \"\"\"\n",
    "            output = '-----'\n",
    "        \"\"\"\n",
    "        We compute tags scores as self.logits, where each element in vector \n",
    "        This is to make a final prediction. Use a fully connected neural network\n",
    "        (a vanilla 189-esque neural net will suffice) to get a vector where each entry\n",
    "        corresponds to a score for a specific tag.\n",
    "        We interpret the ith element as the score of class i for word w.\n",
    "        This has already been done.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            W = tf.get_variable(\"W\", shape=[2*self.config.hidden_size, self.ntags], \n",
    "                dtype=tf.float32)\n",
    "\n",
    "            b = tf.get_variable(\"b\", shape=[self.ntags], dtype=tf.float32, \n",
    "                initializer=tf.zeros_initializer())\n",
    "\n",
    "            ntime_steps = tf.shape(output)[1]\n",
    "            output = tf.reshape(output, [-1, 2*self.config.hidden_size])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            self.logits = tf.reshape(pred, [-1, ntime_steps, self.ntags])\n",
    "\n",
    "\n",
    "    def add_pred_op(self):\n",
    "        \"\"\"\n",
    "        Adds labels_pred to self\n",
    "        \"\"\"\n",
    "        if not self.config.crf:\n",
    "            self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1), tf.int32)\n",
    "\n",
    "    \"\"\"\n",
    "    Step 2.b. We want to compute the probability of a tagging sequence y_t\n",
    "    and find the sequence with the highest probability. here, y_t is the id\n",
    "    of the tag for the t-th word.\n",
    "    \n",
    "    We have two options to make our final prediction:\n",
    "    Softmax: logistic function that squashes scores to range [0,1] that add up to 1.\n",
    "        Calculate losses with tf.nn.sparse_softmax_cross_entropy_with_logits.\n",
    "        Calculate self.loss with tf.reduce_mean to normalize the negative log likelihood\n",
    "            as a probability distribution.\n",
    "        Available parameters: self.logits, self.labels, self.sequence_lengths.\n",
    "    Linear-chain CRF: makes use of the neighboring tag decisions. Finds a sequence of tags\n",
    "    with the best score by scaling scores of intermediate tags with a transition matrix/\n",
    "    probability distribution.\n",
    "        Calculate log_likelihood, self.transition_params with tf.contrib.crf.crf_log_likelihood.\n",
    "        Calculate mask with tf.sequence_mask.\n",
    "        Calculate losses with tf.boolean_mask from previous loss and mask.\n",
    "        Calculate self.loss with tf.reduce_mean to normalize the losses \n",
    "            as a probability distribution.\n",
    "        Available parameters: self.logits, self.labels, self.sequence_lengths.\n",
    "        \n",
    "    \"\"\"\n",
    "    def add_loss_op(self):\n",
    "        \"\"\"\n",
    "        Adds loss to self\n",
    "        \"\"\"\n",
    "        if self.config.crf:\n",
    "            log_likelihood, self.transition_params = '-----'\n",
    "            self.loss = '-----'\n",
    "        else:\n",
    "            losses = '-----'\n",
    "            mask = '-----'\n",
    "            losses = '-----'\n",
    "            self.loss = '-----'\n",
    "\n",
    "        # for tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "\n",
    "\n",
    "    def add_train_op(self):\n",
    "        \"\"\"\n",
    "        Add train_op to self\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            # sgd method\n",
    "            if self.config.lr_method == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            elif self.config.lr_method == 'adagrad':\n",
    "                optimizer = tf.train.AdagradOptimizer(self.lr)\n",
    "            elif self.config.lr_method == 'sgd':\n",
    "                optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            elif self.config.lr_method == 'rmsprop':\n",
    "                optimizer = tf.train.RMSPropOptimizer(self.lr)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown train op {}\".format(\n",
    "                                          self.config.lr_method))\n",
    "\n",
    "            # gradient clipping if config.clip is positive\n",
    "            if self.config.clip > 0:\n",
    "                gradients, variables   = zip(*optimizer.compute_gradients(self.loss))\n",
    "                gradients, global_norm = tf.clip_by_global_norm(gradients, self.config.clip)\n",
    "                self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            else:\n",
    "                self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "    def add_init_op(self):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "    def add_summary(self, sess): \n",
    "        # tensorboard stuff\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.file_writer = tf.summary.FileWriter(self.config.output_path, sess.graph)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.add_word_embeddings_op()\n",
    "        self.add_logits_op()\n",
    "        self.add_pred_op()\n",
    "        self.add_loss_op()\n",
    "        self.add_train_op()\n",
    "        self.add_init_op()\n",
    "\n",
    "\n",
    "    def predict_batch(self, sess, words):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: a tensorflow session\n",
    "            words: list of sentences\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            sequence_length\n",
    "        \"\"\"\n",
    "        # get the feed dictionnary\n",
    "        fd, sequence_lengths = self.get_feed_dict(words, dropout=1.0)\n",
    "\n",
    "        if self.config.crf:\n",
    "            viterbi_sequences = []\n",
    "            logits, transition_params = sess.run([self.logits, self.transition_params], \n",
    "                    feed_dict=fd)\n",
    "            # iterate over the sentences\n",
    "            for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "                # keep only the valid time steps\n",
    "                logit = logit[:sequence_length]\n",
    "                viterbi_sequence, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                                logit, transition_params)\n",
    "                viterbi_sequences += [viterbi_sequence]\n",
    "\n",
    "            return viterbi_sequences, sequence_lengths\n",
    "\n",
    "        else:\n",
    "            labels_pred = sess.run(self.labels_pred, feed_dict=fd)\n",
    "\n",
    "            return labels_pred, sequence_lengths\n",
    "\n",
    "\n",
    "    def run_epoch(self, sess, train, dev, tags, epoch):\n",
    "        \"\"\"\n",
    "        Performs one complete pass over the train set and evaluate on dev\n",
    "        Args:\n",
    "            sess: tensorflow session\n",
    "            train: dataset that yields tuple of sentences, tags\n",
    "            dev: dataset\n",
    "            tags: {tag: index} dictionary\n",
    "            epoch: (int) number of the epoch\n",
    "        \"\"\"\n",
    "        nbatches = (len(train) + self.config.batch_size - 1) // self.config.batch_size\n",
    "        prog = Progbar(target=nbatches)\n",
    "        for i, (words, labels) in enumerate(minibatches(train, self.config.batch_size)):\n",
    "            fd, _ = self.get_feed_dict(words, labels, self.config.lr, self.config.dropout)\n",
    "\n",
    "            _, train_loss, summary = sess.run([self.train_op, self.loss, self.merged], feed_dict=fd)\n",
    "\n",
    "            prog.update(i + 1, [(\"train loss\", train_loss)])\n",
    "\n",
    "            # tensorboard\n",
    "            if i % 10 == 0:\n",
    "                self.file_writer.add_summary(summary, epoch*nbatches + i)\n",
    "\n",
    "        acc, f1 = self.run_evaluate(sess, dev, tags)\n",
    "        self.logger.info(\"- dev acc {:04.2f} - f1 {:04.2f}\".format(100*acc, 100*f1))\n",
    "        return acc, f1\n",
    "\n",
    "\n",
    "    def run_evaluate(self, sess, test, tags):\n",
    "        \"\"\"\n",
    "        Evaluates performance on test set\n",
    "        Args:\n",
    "            sess: tensorflow session\n",
    "            test: dataset that yields tuple of sentences, tags\n",
    "            tags: {tag: index} dictionary\n",
    "        Returns:\n",
    "            accuracy\n",
    "            f1 score\n",
    "        \"\"\"\n",
    "        accs = []\n",
    "        correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "        for words, labels in minibatches(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(sess, words)\n",
    "\n",
    "            for lab, lab_pred, length in zip(labels, labels_pred, sequence_lengths):\n",
    "                lab = lab[:length]\n",
    "                lab_pred = lab_pred[:length]\n",
    "                accs += [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "                lab_chunks = set(get_chunks(lab, tags))\n",
    "                lab_pred_chunks = set(get_chunks(lab_pred, tags))\n",
    "                correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "                total_preds += len(lab_pred_chunks)\n",
    "                total_correct += len(lab_chunks)\n",
    "\n",
    "        p = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "        acc = np.mean(accs)\n",
    "        return acc, f1\n",
    "\n",
    "\n",
    "    def train(self, train, dev, tags):\n",
    "        \"\"\"\n",
    "        Performs training with early stopping and lr exponential decay\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of sentences, tags\n",
    "            dev: dataset\n",
    "            tags: {tag: index} dictionary\n",
    "        \"\"\"\n",
    "        best_score = 0\n",
    "        saver = tf.train.Saver()\n",
    "        # for early stopping\n",
    "        nepoch_no_imprv = 0\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(self.init)\n",
    "            if self.config.reload:\n",
    "                self.logger.info(\"Reloading the latest trained model...\")\n",
    "                saver.restore(sess, self.config.model_output)\n",
    "            # tensorboard\n",
    "            self.add_summary(sess)\n",
    "            for epoch in range(self.config.nepochs):\n",
    "                self.logger.info(\"Epoch {:} out of {:}\".format(epoch + 1, self.config.nepochs))\n",
    "\n",
    "                acc, f1 = self.run_epoch(sess, train, dev, tags, epoch)\n",
    "\n",
    "                # decay learning rate\n",
    "                self.config.lr *= self.config.lr_decay\n",
    "\n",
    "                # early stopping and saving best parameters\n",
    "                if f1 >= best_score:\n",
    "                    nepoch_no_imprv = 0\n",
    "                    if not os.path.exists(self.config.model_output):\n",
    "                        os.makedirs(self.config.model_output)\n",
    "                    saver.save(sess, self.config.model_output)\n",
    "                    best_score = f1\n",
    "                    self.logger.info(\"- new best score!\")\n",
    "\n",
    "                else:\n",
    "                    nepoch_no_imprv += 1\n",
    "                    if nepoch_no_imprv >= self.config.nepoch_no_imprv:\n",
    "                        self.logger.info(\"- early stopping {} epochs without improvement\".format(\n",
    "                                        nepoch_no_imprv))\n",
    "                        break\n",
    "\n",
    "\n",
    "    def evaluate(self, test, tags):\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            self.logger.info(\"Testing model over test set\")\n",
    "            saver.restore(sess, self.config.model_output)\n",
    "            acc, f1 = self.run_evaluate(sess, test, tags)\n",
    "            self.logger.info(\"- test acc {:04.2f} - f1 {:04.2f}\".format(100*acc, 100*f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # load vocabs\n",
    "    vocab_words = load_vocab(config.words_filename)\n",
    "    vocab_tags  = load_vocab(config.tags_filename)\n",
    "    vocab_chars = load_vocab(config.chars_filename)\n",
    "\n",
    "    # get processing functions\n",
    "    processing_word = get_processing_word(vocab_words, vocab_chars,\n",
    "                    lowercase=True, chars=config.chars)\n",
    "    processing_tag  = get_processing_word(vocab_tags, \n",
    "                    lowercase=False)\n",
    "\n",
    "    # get pre trained embeddings\n",
    "    embeddings = get_trimmed_glove_vectors(config.trimmed_filename)\n",
    "\n",
    "    # create dataset\n",
    "    dev   = CoNLLDataset(config.dev_filename, processing_word,\n",
    "                        processing_tag, config.max_iter)\n",
    "    test  = CoNLLDataset(config.test_filename, processing_word,\n",
    "                        processing_tag, config.max_iter)\n",
    "    train = CoNLLDataset(config.train_filename, processing_word,\n",
    "                        processing_tag, config.max_iter)\n",
    "\n",
    "    # build model\n",
    "    model = NERModel(config, embeddings, ntags=len(vocab_tags),\n",
    "                                         nchars=len(vocab_chars))\n",
    "    model.build()\n",
    "\n",
    "    # train, evaluate\n",
    "    model.train(train, dev, vocab_tags)\n",
    "    model.evaluate(test, vocab_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, build vocab from the data and extract trimmed glove vectors according to the config in `config.py`.\n",
    "```\n",
    "python build_data.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create instance of config\n",
    "config = Config()\n",
    "\n",
    "# load, train, evaluate and interact with model\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model and run it in interactive mode on shell:\n",
    "```\n",
    "python main.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
